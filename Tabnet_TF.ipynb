{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load (iris dataset) & Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 125\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    features = tf.unstack(ds['features'])\n",
    "    labels = ds['label']\n",
    "\n",
    "    x = dict(zip(col_names, features))\n",
    "    y = tf.one_hot(labels, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 21:42:17.899037: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-14 21:42:17.899353: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "ds_full = tfds.load(name=\"iris\", split=tfds.Split.TRAIN)\n",
    "ds_full = ds_full.shuffle(150, seed=0)\n",
    "\n",
    "ds_train = ds_full.take(train_size)\n",
    "ds_train = ds_train.map(transform)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "\n",
    "ds_test = ds_full.skip(train_size)\n",
    "ds_test = ds_test.map(transform)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for col_name in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_keras_custom_object(cls):\n",
    "    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sparsemax(logits, axis):\n",
    "    \"\"\"Sparsemax activation function [1].\n",
    "    For each batch `i` and class `j` we have\n",
    "      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n",
    "    [1]: https://arxiv.org/abs/1602.02068\n",
    "    Args:\n",
    "        logits: Input tensor.\n",
    "        axis: Integer, axis along which the sparsemax operation is applied.\n",
    "    Returns:\n",
    "        Tensor, output of sparsemax transformation. Has the same type and\n",
    "        shape as `logits`.\n",
    "    Raises:\n",
    "        ValueError: In case `dim(logits) == 1`.\n",
    "    \"\"\"\n",
    "    logits = tf.convert_to_tensor(logits, name=\"logits\")\n",
    "\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    rank = shape.rank\n",
    "    is_last_axis = (axis == -1) or (axis == rank - 1)\n",
    "\n",
    "    if is_last_axis:\n",
    "        output = _compute_2d_sparsemax(logits)\n",
    "        output.set_shape(shape)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a transpose so that we can\n",
    "    # still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    rank_op = tf.rank(logits)\n",
    "    axis_norm = axis % rank\n",
    "    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = _compute_2d_sparsemax(logits)\n",
    "    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Make shape inference work since transpose may erase its static shape.\n",
    "    output.set_shape(shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_axis(logits, dim_index, last_index, **kwargs):\n",
    "    return tf.transpose(\n",
    "        logits,\n",
    "        tf.concat(\n",
    "            [\n",
    "                tf.range(dim_index),\n",
    "                [last_index],\n",
    "                tf.range(dim_index + 1, last_index),\n",
    "                [dim_index],\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_2d_sparsemax(logits):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            groups: int = 2,\n",
    "            axis: int = -1,\n",
    "            epsilon: float = 1e-3,\n",
    "            center: bool = True,\n",
    "            scale: bool = True,\n",
    "            beta_initializer=\"zeros\",\n",
    "            gamma_initializer=\"ones\",\n",
    "            beta_regularizer=None,\n",
    "            gamma_regularizer=None,\n",
    "            beta_constraint=None,\n",
    "            gamma_constraint=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Training=none is just for compat with batchnorm signature call\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(\n",
    "            inputs, input_shape, tensor_input_shape\n",
    "        )\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(\n",
    "                self.gamma_initializer\n",
    "            ),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n",
    "                self.gamma_regularizer\n",
    "            ),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(\n",
    "            reshaped_inputs, group_reduction_axes, keepdims=True\n",
    "        )\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                                           \"input tensor should have a defined dimension \"\n",
    "                                           \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                                                          \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                                                          \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            ndim=len(input_shape), axes={self.axis: dim}\n",
    "        )\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, features,\n",
    "                 norm_type,\n",
    "                 momentum=0.9,\n",
    "                 virtual_batch_size=None,\n",
    "                 groups=2,\n",
    "                 block_name='',\n",
    "                 **kwargs):\n",
    "        super(TransformBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = features\n",
    "        self.norm_type = norm_type\n",
    "        self.momentum = momentum\n",
    "        self.groups = groups\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n",
    "\n",
    "        if norm_type == 'batch':\n",
    "            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum,\n",
    "                                                         virtual_batch_size=virtual_batch_size,\n",
    "                                                         name=f'transformblock_bn_{block_name}')\n",
    "\n",
    "        else:\n",
    "            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f'transformblock_gn_{block_name}')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.transform(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TabNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_features=None,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=2,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__(**kwargs)\n",
    "        if random_state is not None:\n",
    "            tf.keras.utils.set_random_seed(random_state)\n",
    "\n",
    "        # Input checks\n",
    "        if feature_columns is not None:\n",
    "            if type(feature_columns) not in (list, tuple):\n",
    "                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n",
    "\n",
    "            if len(feature_columns) == 0:\n",
    "                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n",
    "\n",
    "            if num_features is None:\n",
    "                num_features = len(feature_columns)\n",
    "            else:\n",
    "                num_features = int(num_features)\n",
    "\n",
    "        else:\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n",
    "\n",
    "        if num_decision_steps < 1:\n",
    "            raise ValueError(\"Num decision steps must be greater than 0.\")\n",
    "\n",
    "        if feature_dim <= output_dim:\n",
    "            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n",
    "\n",
    "        feature_dim = int(feature_dim)\n",
    "        output_dim = int(output_dim)\n",
    "        num_decision_steps = int(num_decision_steps)\n",
    "        relaxation_factor = float(relaxation_factor)\n",
    "        sparsity_coefficient = float(sparsity_coefficient)\n",
    "        batch_momentum = float(batch_momentum)\n",
    "        num_groups = max(1, int(num_groups))\n",
    "        epsilon = float(epsilon)\n",
    "\n",
    "        if relaxation_factor < 0.:\n",
    "            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n",
    "\n",
    "        if sparsity_coefficient < 0.:\n",
    "            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n",
    "\n",
    "        if virtual_batch_size is not None:\n",
    "            virtual_batch_size = int(virtual_batch_size)\n",
    "\n",
    "        if norm_type not in ['batch', 'group']:\n",
    "            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n",
    "\n",
    "        self.feature_columns = feature_columns\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "        self.norm_type = norm_type\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if num_decision_steps > 1:\n",
    "            features_for_coeff = feature_dim - output_dim\n",
    "            print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n",
    "\n",
    "            if self.norm_type == 'batch':\n",
    "                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n",
    "            else:\n",
    "                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name='input_gn')\n",
    "\n",
    "        else:\n",
    "            self.input_features = None\n",
    "            self.input_bn = None\n",
    "\n",
    "        self.transform_f1 = TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n",
    "                                           block_name='f1')\n",
    "\n",
    "        self.transform_f2 = TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n",
    "                                           block_name='f2')\n",
    "\n",
    "        self.transform_f3_list = [\n",
    "            TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f3_{i}')\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_f4_list = [\n",
    "            TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f4_{i}')\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_coef_list = [\n",
    "            TransformBlock(self.num_features, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'coef_{i}')\n",
    "            for i in range(self.num_decision_steps - 1)\n",
    "        ]\n",
    "\n",
    "        self._step_feature_selection_masks = None\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.input_features is not None:\n",
    "            features = self.input_features(inputs)\n",
    "            features = self.input_bn(features, training=training)\n",
    "\n",
    "        else:\n",
    "            features = inputs\n",
    "\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        self._step_feature_selection_masks = []\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "        # Initializes decision-step dependent variables.\n",
    "        output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
    "        masked_features = features\n",
    "        mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        complementary_aggregated_mask_values = tf.ones(\n",
    "            [batch_size, self.num_features])\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        entropy_loss = 0.\n",
    "\n",
    "        for ni in range(self.num_decision_steps):\n",
    "            # Feature transformer with two shared and two decision step dependent\n",
    "            # blocks is used below.=\n",
    "            transform_f1 = self.transform_f1(masked_features, training=training)\n",
    "            transform_f1 = glu(transform_f1, self.feature_dim)\n",
    "\n",
    "            transform_f2 = self.transform_f2(transform_f1, training=training)\n",
    "            transform_f2 = (glu(transform_f2, self.feature_dim) +\n",
    "                            transform_f1) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n",
    "            transform_f3 = (glu(transform_f3, self.feature_dim) +\n",
    "                            transform_f2) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n",
    "            transform_f4 = (glu(transform_f4, self.feature_dim) +\n",
    "                            transform_f3) * tf.math.sqrt(0.5)\n",
    "\n",
    "            if (ni > 0 or self.num_decision_steps == 1):\n",
    "                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n",
    "\n",
    "                # Decision aggregation.\n",
    "                output_aggregated += decision_out\n",
    "\n",
    "                # Aggregated masks are used for visualization of the\n",
    "                # feature importance attributes.\n",
    "                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n",
    "\n",
    "                if self.num_decision_steps > 1:\n",
    "                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n",
    "\n",
    "                aggregated_mask_values += mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = transform_f4[:, self.output_dim:]\n",
    "\n",
    "            if ni < (self.num_decision_steps - 1):\n",
    "                # Determines the feature masks via linear and nonlinear\n",
    "                # transformations, taking into account of aggregated feature use.\n",
    "                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n",
    "                mask_values *= complementary_aggregated_mask_values\n",
    "                mask_values = sparsemax(mask_values, axis=-1)\n",
    "\n",
    "                # Relaxation factor controls the amount of reuse of features between\n",
    "                # different decision blocks and updated with the values of\n",
    "                # coefficients.\n",
    "                complementary_aggregated_mask_values *= (\n",
    "                        self.relaxation_factor - mask_values)\n",
    "\n",
    "                # Entropy is used to penalize the amount of sparsity in feature\n",
    "                # selection.\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) / (\n",
    "                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n",
    "\n",
    "                # Add entropy loss\n",
    "                entropy_loss = total_entropy\n",
    "\n",
    "                # Feature selection.\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # Visualization of the feature selection mask at decision step ni\n",
    "                # tf.summary.image(\n",
    "                #     \"Mask for step\" + str(ni),\n",
    "                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
    "                #     max_outputs=1)\n",
    "                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n",
    "                self._step_feature_selection_masks.append(mask_at_step_i)\n",
    "\n",
    "            else:\n",
    "                # This branch is needed for correct compilation by tf.autograph\n",
    "                entropy_loss = 0.\n",
    "\n",
    "        # Adds the loss automatically\n",
    "        self.add_loss(self.sparsity_coefficient * entropy_loss)\n",
    "\n",
    "        # Visualization of the aggregated feature importances\n",
    "        # tf.summary.image(\n",
    "        #     \"Aggregated mask\",\n",
    "        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
    "        #     max_outputs=1)\n",
    "\n",
    "        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n",
    "        self._step_aggregate_feature_selection_mask = agg_mask\n",
    "\n",
    "        return output_aggregated\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return self._step_feature_selection_masks\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return self._step_aggregate_feature_selection_mask\n",
    "\n",
    "\n",
    "class TabNetClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 num_classes,\n",
    "                 num_features=None,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=1,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.tabnet = TabNet(feature_columns=feature_columns,\n",
    "                             num_features=num_features,\n",
    "                             feature_dim=feature_dim,\n",
    "                             output_dim=output_dim,\n",
    "                             num_decision_steps=num_decision_steps,\n",
    "                             relaxation_factor=relaxation_factor,\n",
    "                             sparsity_coefficient=sparsity_coefficient,\n",
    "                             norm_type=norm_type,\n",
    "                             batch_momentum=batch_momentum,\n",
    "                             virtual_batch_size=virtual_batch_size,\n",
    "                             num_groups=num_groups,\n",
    "                             epsilon=epsilon,\n",
    "                             random_state=random_state,\n",
    "                             **kwargs)\n",
    "\n",
    "        self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False, name='classifier')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)\n",
    "\n",
    "\n",
    "class TabNetRegressor(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 num_regressors,\n",
    "                 num_features=None,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=1,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regression variables.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.tabnet = TabNet(feature_columns=feature_columns,\n",
    "                             num_features=num_features,\n",
    "                             feature_dim=feature_dim,\n",
    "                             output_dim=output_dim,\n",
    "                             num_decision_steps=num_decision_steps,\n",
    "                             relaxation_factor=relaxation_factor,\n",
    "                             sparsity_coefficient=sparsity_coefficient,\n",
    "                             norm_type=norm_type,\n",
    "                             batch_momentum=batch_momentum,\n",
    "                             virtual_batch_size=virtual_batch_size,\n",
    "                             num_groups=num_groups,\n",
    "                             epsilon=epsilon,\n",
    "                             random_state=random_state,\n",
    "                             **kwargs)\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name='regressor')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 4 features will be used for decision steps.\n"
     ]
    }
   ],
   "source": [
    "# Group Norm does better for small datasets\n",
    "model = TabNetClassifier(feature_columns, num_classes=3,\n",
    "                        feature_dim=8, output_dim=4,\n",
    "                        num_decision_steps=4, relaxation_factor=1.0,\n",
    "                        sparsity_coefficient=1e-5, batch_momentum=0.98,\n",
    "                        virtual_batch_size=None, norm_type='group',\n",
    "                        num_groups=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.9, staircase=False)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(lr)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 21:42:20.110146: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-14 21:42:22.820404: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-14 21:42:28.048844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 9s - loss: 1.1444 - accuracy: 0.5200 - val_loss: 0.9130 - val_accuracy: 0.6400 - 9s/epoch - 3s/step\n",
      "Epoch 2/100\n",
      "3/3 - 1s - loss: 0.8629 - accuracy: 0.6240 - val_loss: 0.7053 - val_accuracy: 0.6400 - 673ms/epoch - 224ms/step\n",
      "Epoch 3/100\n",
      "3/3 - 1s - loss: 0.7504 - accuracy: 0.6480 - val_loss: 0.5946 - val_accuracy: 0.8000 - 552ms/epoch - 184ms/step\n",
      "Epoch 4/100\n",
      "3/3 - 1s - loss: 0.6968 - accuracy: 0.6560 - val_loss: 0.7540 - val_accuracy: 0.6000 - 619ms/epoch - 206ms/step\n",
      "Epoch 5/100\n",
      "3/3 - 1s - loss: 0.6248 - accuracy: 0.6640 - val_loss: 0.6288 - val_accuracy: 0.6400 - 640ms/epoch - 213ms/step\n",
      "Epoch 6/100\n",
      "3/3 - 0s - loss: 0.5777 - accuracy: 0.6640 - val_loss: 0.5976 - val_accuracy: 0.7200 - 471ms/epoch - 157ms/step\n",
      "Epoch 7/100\n",
      "3/3 - 1s - loss: 0.5123 - accuracy: 0.7040 - val_loss: 0.4591 - val_accuracy: 0.8400 - 695ms/epoch - 232ms/step\n",
      "Epoch 8/100\n",
      "3/3 - 0s - loss: 0.5038 - accuracy: 0.8240 - val_loss: 0.4627 - val_accuracy: 0.9600 - 440ms/epoch - 147ms/step\n",
      "Epoch 9/100\n",
      "3/3 - 1s - loss: 0.4559 - accuracy: 0.8960 - val_loss: 0.3844 - val_accuracy: 0.9600 - 538ms/epoch - 179ms/step\n",
      "Epoch 10/100\n",
      "3/3 - 0s - loss: 0.3812 - accuracy: 0.9200 - val_loss: 0.3919 - val_accuracy: 0.8800 - 351ms/epoch - 117ms/step\n",
      "Epoch 11/100\n",
      "3/3 - 1s - loss: 0.3068 - accuracy: 0.9360 - val_loss: 0.3776 - val_accuracy: 0.8800 - 587ms/epoch - 196ms/step\n",
      "Epoch 12/100\n",
      "3/3 - 0s - loss: 0.4008 - accuracy: 0.8320 - val_loss: 0.2078 - val_accuracy: 0.9600 - 397ms/epoch - 132ms/step\n",
      "Epoch 13/100\n",
      "3/3 - 0s - loss: 0.2538 - accuracy: 0.9120 - val_loss: 0.3784 - val_accuracy: 0.8400 - 243ms/epoch - 81ms/step\n",
      "Epoch 14/100\n",
      "3/3 - 0s - loss: 0.2332 - accuracy: 0.9200 - val_loss: 0.3235 - val_accuracy: 0.8800 - 310ms/epoch - 103ms/step\n",
      "Epoch 15/100\n",
      "3/3 - 0s - loss: 0.2692 - accuracy: 0.8960 - val_loss: 0.1424 - val_accuracy: 0.9600 - 283ms/epoch - 94ms/step\n",
      "Epoch 16/100\n",
      "3/3 - 0s - loss: 0.1752 - accuracy: 0.9280 - val_loss: 0.2041 - val_accuracy: 0.9600 - 259ms/epoch - 86ms/step\n",
      "Epoch 17/100\n",
      "3/3 - 0s - loss: 0.1851 - accuracy: 0.9280 - val_loss: 0.3127 - val_accuracy: 0.8800 - 237ms/epoch - 79ms/step\n",
      "Epoch 18/100\n",
      "3/3 - 0s - loss: 0.2020 - accuracy: 0.9200 - val_loss: 0.3119 - val_accuracy: 0.8400 - 291ms/epoch - 97ms/step\n",
      "Epoch 19/100\n",
      "3/3 - 0s - loss: 0.1552 - accuracy: 0.9360 - val_loss: 0.1468 - val_accuracy: 0.9200 - 295ms/epoch - 98ms/step\n",
      "Epoch 20/100\n",
      "3/3 - 1s - loss: 0.1720 - accuracy: 0.9360 - val_loss: 0.0810 - val_accuracy: 0.9600 - 501ms/epoch - 167ms/step\n",
      "Epoch 21/100\n",
      "3/3 - 0s - loss: 0.2067 - accuracy: 0.9280 - val_loss: 0.0948 - val_accuracy: 0.9600 - 249ms/epoch - 83ms/step\n",
      "Epoch 22/100\n",
      "3/3 - 0s - loss: 0.2292 - accuracy: 0.8960 - val_loss: 0.0804 - val_accuracy: 0.9600 - 242ms/epoch - 81ms/step\n",
      "Epoch 23/100\n",
      "3/3 - 0s - loss: 0.1726 - accuracy: 0.9360 - val_loss: 0.3665 - val_accuracy: 0.8400 - 279ms/epoch - 93ms/step\n",
      "Epoch 24/100\n",
      "3/3 - 0s - loss: 0.1647 - accuracy: 0.9440 - val_loss: 0.1348 - val_accuracy: 0.9600 - 343ms/epoch - 114ms/step\n",
      "Epoch 25/100\n",
      "3/3 - 0s - loss: 0.1834 - accuracy: 0.9280 - val_loss: 0.2625 - val_accuracy: 0.8800 - 281ms/epoch - 94ms/step\n",
      "Epoch 26/100\n",
      "3/3 - 0s - loss: 0.1945 - accuracy: 0.9200 - val_loss: 0.0994 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 27/100\n",
      "3/3 - 0s - loss: 0.1510 - accuracy: 0.9360 - val_loss: 0.0812 - val_accuracy: 1.0000 - 243ms/epoch - 81ms/step\n",
      "Epoch 28/100\n",
      "3/3 - 0s - loss: 0.1614 - accuracy: 0.9440 - val_loss: 0.1610 - val_accuracy: 0.9200 - 277ms/epoch - 92ms/step\n",
      "Epoch 29/100\n",
      "3/3 - 0s - loss: 0.3030 - accuracy: 0.8880 - val_loss: 0.0901 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 30/100\n",
      "3/3 - 0s - loss: 0.1952 - accuracy: 0.9360 - val_loss: 0.4003 - val_accuracy: 0.8000 - 278ms/epoch - 93ms/step\n",
      "Epoch 31/100\n",
      "3/3 - 0s - loss: 0.2901 - accuracy: 0.9040 - val_loss: 0.3231 - val_accuracy: 0.8800 - 235ms/epoch - 78ms/step\n",
      "Epoch 32/100\n",
      "3/3 - 0s - loss: 0.2626 - accuracy: 0.8960 - val_loss: 0.2743 - val_accuracy: 0.8800 - 238ms/epoch - 79ms/step\n",
      "Epoch 33/100\n",
      "3/3 - 0s - loss: 0.2060 - accuracy: 0.9120 - val_loss: 0.2453 - val_accuracy: 0.8800 - 238ms/epoch - 79ms/step\n",
      "Epoch 34/100\n",
      "3/3 - 0s - loss: 0.2633 - accuracy: 0.8960 - val_loss: 0.2310 - val_accuracy: 0.8800 - 239ms/epoch - 80ms/step\n",
      "Epoch 35/100\n",
      "3/3 - 0s - loss: 0.1925 - accuracy: 0.9120 - val_loss: 0.3744 - val_accuracy: 0.8400 - 241ms/epoch - 80ms/step\n",
      "Epoch 36/100\n",
      "3/3 - 0s - loss: 0.1605 - accuracy: 0.9600 - val_loss: 0.0857 - val_accuracy: 0.9600 - 235ms/epoch - 78ms/step\n",
      "Epoch 37/100\n",
      "3/3 - 0s - loss: 0.1907 - accuracy: 0.9360 - val_loss: 0.1926 - val_accuracy: 0.9200 - 239ms/epoch - 80ms/step\n",
      "Epoch 38/100\n",
      "3/3 - 0s - loss: 0.1702 - accuracy: 0.9360 - val_loss: 0.2479 - val_accuracy: 0.8400 - 297ms/epoch - 99ms/step\n",
      "Epoch 39/100\n",
      "3/3 - 0s - loss: 0.1641 - accuracy: 0.9200 - val_loss: 0.1269 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 40/100\n",
      "3/3 - 0s - loss: 0.1523 - accuracy: 0.9360 - val_loss: 0.0606 - val_accuracy: 0.9600 - 235ms/epoch - 78ms/step\n",
      "Epoch 41/100\n",
      "3/3 - 0s - loss: 0.1614 - accuracy: 0.9440 - val_loss: 0.0914 - val_accuracy: 1.0000 - 239ms/epoch - 80ms/step\n",
      "Epoch 42/100\n",
      "3/3 - 0s - loss: 0.1252 - accuracy: 0.9600 - val_loss: 0.1678 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 43/100\n",
      "3/3 - 0s - loss: 0.1702 - accuracy: 0.9280 - val_loss: 0.1448 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 44/100\n",
      "3/3 - 0s - loss: 0.1642 - accuracy: 0.9360 - val_loss: 0.1396 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 45/100\n",
      "3/3 - 0s - loss: 0.1366 - accuracy: 0.9360 - val_loss: 0.0386 - val_accuracy: 1.0000 - 236ms/epoch - 79ms/step\n",
      "Epoch 46/100\n",
      "3/3 - 0s - loss: 0.1439 - accuracy: 0.9200 - val_loss: 0.0725 - val_accuracy: 0.9600 - 239ms/epoch - 80ms/step\n",
      "Epoch 47/100\n",
      "3/3 - 0s - loss: 0.1093 - accuracy: 0.9600 - val_loss: 0.0258 - val_accuracy: 1.0000 - 233ms/epoch - 78ms/step\n",
      "Epoch 48/100\n",
      "3/3 - 0s - loss: 0.1683 - accuracy: 0.9200 - val_loss: 0.2269 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 49/100\n",
      "3/3 - 0s - loss: 0.1303 - accuracy: 0.9760 - val_loss: 0.2712 - val_accuracy: 0.8800 - 240ms/epoch - 80ms/step\n",
      "Epoch 50/100\n",
      "3/3 - 0s - loss: 0.1612 - accuracy: 0.9360 - val_loss: 0.2469 - val_accuracy: 0.9200 - 240ms/epoch - 80ms/step\n",
      "Epoch 51/100\n",
      "3/3 - 0s - loss: 0.1963 - accuracy: 0.9200 - val_loss: 0.1256 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 52/100\n",
      "3/3 - 0s - loss: 0.1362 - accuracy: 0.9520 - val_loss: 0.0768 - val_accuracy: 0.9600 - 243ms/epoch - 81ms/step\n",
      "Epoch 53/100\n",
      "3/3 - 0s - loss: 0.1937 - accuracy: 0.9280 - val_loss: 0.0957 - val_accuracy: 0.9600 - 236ms/epoch - 79ms/step\n",
      "Epoch 54/100\n",
      "3/3 - 0s - loss: 0.1693 - accuracy: 0.9360 - val_loss: 0.1501 - val_accuracy: 0.9600 - 240ms/epoch - 80ms/step\n",
      "Epoch 55/100\n",
      "3/3 - 0s - loss: 0.1385 - accuracy: 0.9440 - val_loss: 0.1079 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 56/100\n",
      "3/3 - 0s - loss: 0.1751 - accuracy: 0.9200 - val_loss: 0.0447 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 57/100\n",
      "3/3 - 0s - loss: 0.1293 - accuracy: 0.9440 - val_loss: 0.1855 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 58/100\n",
      "3/3 - 0s - loss: 0.1220 - accuracy: 0.9440 - val_loss: 0.1012 - val_accuracy: 0.9600 - 247ms/epoch - 82ms/step\n",
      "Epoch 59/100\n",
      "3/3 - 0s - loss: 0.1184 - accuracy: 0.9520 - val_loss: 0.0859 - val_accuracy: 0.9600 - 240ms/epoch - 80ms/step\n",
      "Epoch 60/100\n",
      "3/3 - 0s - loss: 0.1378 - accuracy: 0.9440 - val_loss: 0.3444 - val_accuracy: 0.8400 - 237ms/epoch - 79ms/step\n",
      "Epoch 61/100\n",
      "3/3 - 0s - loss: 0.1316 - accuracy: 0.9520 - val_loss: 0.1612 - val_accuracy: 0.9600 - 241ms/epoch - 80ms/step\n",
      "Epoch 62/100\n",
      "3/3 - 0s - loss: 0.1111 - accuracy: 0.9600 - val_loss: 0.1396 - val_accuracy: 0.9200 - 240ms/epoch - 80ms/step\n",
      "Epoch 63/100\n",
      "3/3 - 0s - loss: 0.1586 - accuracy: 0.9360 - val_loss: 0.0168 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 64/100\n",
      "3/3 - 0s - loss: 0.1193 - accuracy: 0.9520 - val_loss: 0.0345 - val_accuracy: 1.0000 - 241ms/epoch - 80ms/step\n",
      "Epoch 65/100\n",
      "3/3 - 0s - loss: 0.1796 - accuracy: 0.9200 - val_loss: 0.1334 - val_accuracy: 0.9600 - 235ms/epoch - 78ms/step\n",
      "Epoch 66/100\n",
      "3/3 - 0s - loss: 0.1112 - accuracy: 0.9680 - val_loss: 0.0623 - val_accuracy: 0.9600 - 241ms/epoch - 80ms/step\n",
      "Epoch 67/100\n",
      "3/3 - 0s - loss: 0.1153 - accuracy: 0.9680 - val_loss: 0.0460 - val_accuracy: 0.9600 - 236ms/epoch - 79ms/step\n",
      "Epoch 68/100\n",
      "3/3 - 0s - loss: 0.1386 - accuracy: 0.9280 - val_loss: 0.1297 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 69/100\n",
      "3/3 - 0s - loss: 0.1130 - accuracy: 0.9600 - val_loss: 0.1315 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 70/100\n",
      "3/3 - 0s - loss: 0.0887 - accuracy: 0.9680 - val_loss: 0.0444 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 71/100\n",
      "3/3 - 0s - loss: 0.1121 - accuracy: 0.9680 - val_loss: 0.1748 - val_accuracy: 0.9600 - 241ms/epoch - 80ms/step\n",
      "Epoch 72/100\n",
      "3/3 - 0s - loss: 0.1084 - accuracy: 0.9680 - val_loss: 0.0677 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 73/100\n",
      "3/3 - 0s - loss: 0.1263 - accuracy: 0.9440 - val_loss: 0.1525 - val_accuracy: 0.9600 - 236ms/epoch - 79ms/step\n",
      "Epoch 74/100\n",
      "3/3 - 0s - loss: 0.2341 - accuracy: 0.9120 - val_loss: 0.2602 - val_accuracy: 0.8400 - 241ms/epoch - 80ms/step\n",
      "Epoch 75/100\n",
      "3/3 - 0s - loss: 0.1961 - accuracy: 0.9120 - val_loss: 0.2295 - val_accuracy: 0.9200 - 235ms/epoch - 78ms/step\n",
      "Epoch 76/100\n",
      "3/3 - 0s - loss: 0.1936 - accuracy: 0.9040 - val_loss: 0.1587 - val_accuracy: 0.9600 - 239ms/epoch - 80ms/step\n",
      "Epoch 77/100\n",
      "3/3 - 0s - loss: 0.1338 - accuracy: 0.9440 - val_loss: 0.0191 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 78/100\n",
      "3/3 - 0s - loss: 0.0740 - accuracy: 0.9840 - val_loss: 0.3534 - val_accuracy: 0.9200 - 240ms/epoch - 80ms/step\n",
      "Epoch 79/100\n",
      "3/3 - 0s - loss: 0.2679 - accuracy: 0.9200 - val_loss: 0.0729 - val_accuracy: 0.9600 - 239ms/epoch - 80ms/step\n",
      "Epoch 80/100\n",
      "3/3 - 0s - loss: 0.1515 - accuracy: 0.9600 - val_loss: 0.1219 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 81/100\n",
      "3/3 - 0s - loss: 0.1952 - accuracy: 0.9360 - val_loss: 0.1207 - val_accuracy: 0.9600 - 235ms/epoch - 78ms/step\n",
      "Epoch 82/100\n",
      "3/3 - 0s - loss: 0.1540 - accuracy: 0.9360 - val_loss: 0.1817 - val_accuracy: 0.9200 - 240ms/epoch - 80ms/step\n",
      "Epoch 83/100\n",
      "3/3 - 0s - loss: 0.1237 - accuracy: 0.9520 - val_loss: 0.2273 - val_accuracy: 0.9600 - 239ms/epoch - 80ms/step\n",
      "Epoch 84/100\n",
      "3/3 - 0s - loss: 0.1335 - accuracy: 0.9680 - val_loss: 0.1299 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 85/100\n",
      "3/3 - 0s - loss: 0.1409 - accuracy: 0.9600 - val_loss: 0.2356 - val_accuracy: 0.9200 - 238ms/epoch - 79ms/step\n",
      "Epoch 86/100\n",
      "3/3 - 0s - loss: 0.1356 - accuracy: 0.9440 - val_loss: 0.1758 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 87/100\n",
      "3/3 - 0s - loss: 0.1288 - accuracy: 0.9440 - val_loss: 0.0485 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 88/100\n",
      "3/3 - 0s - loss: 0.1190 - accuracy: 0.9600 - val_loss: 0.1215 - val_accuracy: 0.9600 - 236ms/epoch - 79ms/step\n",
      "Epoch 89/100\n",
      "3/3 - 0s - loss: 0.0976 - accuracy: 0.9760 - val_loss: 0.0313 - val_accuracy: 1.0000 - 241ms/epoch - 80ms/step\n",
      "Epoch 90/100\n",
      "3/3 - 0s - loss: 0.0971 - accuracy: 0.9760 - val_loss: 0.0812 - val_accuracy: 0.9600 - 237ms/epoch - 79ms/step\n",
      "Epoch 91/100\n",
      "3/3 - 0s - loss: 0.1069 - accuracy: 0.9600 - val_loss: 0.1395 - val_accuracy: 0.9600 - 245ms/epoch - 82ms/step\n",
      "Epoch 92/100\n",
      "3/3 - 0s - loss: 0.1010 - accuracy: 0.9600 - val_loss: 0.0453 - val_accuracy: 0.9600 - 249ms/epoch - 83ms/step\n",
      "Epoch 93/100\n",
      "3/3 - 0s - loss: 0.1117 - accuracy: 0.9600 - val_loss: 0.0542 - val_accuracy: 1.0000 - 238ms/epoch - 79ms/step\n",
      "Epoch 94/100\n",
      "3/3 - 0s - loss: 0.0819 - accuracy: 0.9680 - val_loss: 0.2748 - val_accuracy: 0.9200 - 237ms/epoch - 79ms/step\n",
      "Epoch 95/100\n",
      "3/3 - 0s - loss: 0.0633 - accuracy: 0.9840 - val_loss: 0.0480 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 96/100\n",
      "3/3 - 0s - loss: 0.0858 - accuracy: 0.9680 - val_loss: 0.0690 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n",
      "Epoch 97/100\n",
      "3/3 - 0s - loss: 0.0797 - accuracy: 0.9760 - val_loss: 0.0162 - val_accuracy: 1.0000 - 237ms/epoch - 79ms/step\n",
      "Epoch 98/100\n",
      "3/3 - 0s - loss: 0.1013 - accuracy: 0.9680 - val_loss: 0.0796 - val_accuracy: 0.9600 - 234ms/epoch - 78ms/step\n",
      "Epoch 99/100\n",
      "3/3 - 0s - loss: 0.1241 - accuracy: 0.9520 - val_loss: 0.1024 - val_accuracy: 0.9200 - 236ms/epoch - 79ms/step\n",
      "Epoch 100/100\n",
      "3/3 - 0s - loss: 0.0985 - accuracy: 0.9520 - val_loss: 0.1658 - val_accuracy: 0.9600 - 238ms/epoch - 79ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x292b32b00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tab_net (TabNet)            multiple                  1616      \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,628\n",
      "Trainable params: 1,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"tab_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_features (DenseFeatur  multiple                 0         \n",
      " es)                                                             \n",
      "                                                                 \n",
      " input_gn (GroupNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " transform_block (TransformB  multiple                 96        \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " transform_block_1 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_2 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_3 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_4 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_5 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_6 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_7 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_8 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_9 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_10 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      " transform_block_11 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      " transform_block_12 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,616\n",
      "Trainable params: 1,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 21:42:57.720408: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "pred = model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
